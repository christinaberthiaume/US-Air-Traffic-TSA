{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9e1a027e",
      "metadata": {
        "id": "9e1a027e"
      },
      "source": [
        "# Time Series Analysis of U.S. Air Traffic Data\n",
        "\n",
        "## Abstract\n",
        "\n",
        "The COVID-19 pandemic affected our lives in many ways, one of the biggest being air travel. Restrictions on travel forced many people to stay home, and as a result, the air traffic industry suffered for about a year. Now, in 2024, travel restrictions have been lifted, but how has the pandemic affected air traffic today? This is the research question we will be exploring in this paper. We will be doing a Time Series Analysis on U.S. Air Traffic data from the years 2003 to 2023. We will build a SARIMA model and an LSTM model to forecast the Number of Passengers in upcoming months. We will also run a t-test to determine if there is a difference in the number of p assengers in the period before the pandemic versus the period after the pandemic.\n",
        "\n",
        "*Keywords: Time Series Analysis, SARIMA, Paired t-test, Long-Short Term Memory (LSTM), Neural Networks.*\n",
        "\n",
        "## Overview and Motivation\n",
        "\n",
        "The goal of this project is to explore the trends and seasonality of U.S. air traffic data and to build a forecasting model that can predict the number of passengers in future months. We would like to determine if there is a difference in the number of passengers before the COVID-19 pandemic versus after the pandemic.\n",
        "\n",
        "## Related Work\n",
        "\n",
        "This research question was inspired by my own life. Recently, people around me have been taking a lot of trips, seemingly more than before the pandemic. I found this dataset and thought it would be interesting to see if there is actually a statistically significant difference in air travel before the pandemic versus after.\n",
        "\n",
        "## Data\n",
        "\n",
        "The dataset we used for this project provides details on the monthly U.S. airline traffic from 2003 to 2023 for all commercial U.S. air carriers. The data comes from the U.S. Department of Transportation’s (DOT) Bureau of Transportation Statistics. The dataset contains 250 observations (each month from the years 2003 to 2023) and has 17 columns. For each month, the dataset contains information on the number of domestic air travel passengers, the number of international air travel passengers, the number of domestic flights, the number of international flights, the number of passengers and the distance flown (Revenue Passenger-miles) for both domestic and international flights, the number of seats and the distance flown (Available Seat-miles) for both domestic and international flights, and the passenger-miles as a proportion of available seat-miles (Load Factor) for both domestic and international flights.\n",
        "\n",
        "Data cleaning involved transforming data types of multiple columns. We had to remove commas from the values in a few columns to convert them from strings to floats. We also label encoded the years column (0 to 20). For the LSTM model we also normalized the data so that it was in the range 0 to 1.\n",
        "\n",
        "Source: https://www.kaggle.com/datasets/yyxian/u-s-airline-traffic-data\n",
        "\n",
        "## Final Research Question\n",
        "\n",
        "The research question for this project is *How has the COVID-19 pandemic affected the number of passengers on U.S. flights? Can we accurately forecast the number of passengers in future months?* Our independent variable is time and our dependent variable is the number of passengers. Our null hypothesis is that there is a difference in the number of passengers on U.S. flights before the pandemic versus after the pandemic. The alternative hypothesis is that there is no difference in the number of passengers on U.S. flights. We would also like to train a forecasting model to predict the number of passengers on U.S. flights in coming months. We will use the mean absolute percentage error (MAPE) as an evaluation metric.\n",
        "\n",
        "This research question hasn't changed much over the semester; it was mostly just refined. At first, I thought we might look at domestic versus international flights as well, but it turned out there was much less international data. I also decided to focus on the number of passengers (Pax) variable, as this feature seems to be the most representative of our research question.\n",
        "\n",
        "## Exploratory Data Analysis\n",
        "\n",
        "The first step in Exploratory Data Analysis is taking a look at the data and determining what kind of cleaning, if any, needs to be done."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "deNDztunc3wS"
      },
      "id": "deNDztunc3wS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "409978f3",
      "metadata": {
        "id": "409978f3"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
        "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
        "from sklearn.metrics import mean_absolute_percentage_error\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf, plot_predict\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "import scipy.stats as stats\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import LSTM\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read data in and look at first five rows\n",
        "df = pd.read_csv('/content/drive/MyDrive/Grad school/Spring 2024/CS 539 - Machine Learning/Data/air_traffic.csv')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "ebj-8G-rbJps"
      },
      "id": "ebj-8G-rbJps",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "3aafe718",
      "metadata": {
        "id": "3aafe718"
      },
      "source": [
        "We will check if there are any missing values. We will also make sure all the feature are the correct data types."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b2b036c",
      "metadata": {
        "id": "4b2b036c"
      },
      "outputs": [],
      "source": [
        "# Check for missing values\n",
        "df.isnull().values.any()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0835168",
      "metadata": {
        "id": "b0835168"
      },
      "outputs": [],
      "source": [
        "# Check data types\n",
        "df.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb7675af",
      "metadata": {
        "id": "eb7675af"
      },
      "outputs": [],
      "source": [
        "# Convert objects to floats\n",
        "for col in ['Dom_Pax', 'Int_Pax', 'Pax', 'Dom_Flt', 'Int_Flt', 'Flt', 'Dom_RPM', 'Int_RPM', 'RPM', 'Dom_ASM', 'Int_ASM', 'ASM']:\n",
        "    df[col] = df[col].str.replace(',', '').astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6118dfd0",
      "metadata": {
        "id": "6118dfd0"
      },
      "outputs": [],
      "source": [
        "# Create column Dates for Month - Year pairs\n",
        "months = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']\n",
        "dates = []\n",
        "\n",
        "for year in range(2003, 2024):\n",
        "    for month in months:\n",
        "        dates.append(month + ' ' + str(year))\n",
        "\n",
        "df['Date'] = dates[:249]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1be367e",
      "metadata": {
        "id": "d1be367e"
      },
      "outputs": [],
      "source": [
        "# Encode years\n",
        "le = LabelEncoder()\n",
        "df['Year'] = le.fit_transform(df['Year'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b250caad",
      "metadata": {
        "id": "b250caad"
      },
      "source": [
        "Now that the data is clean, let's look at some summary statistics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2a6f962",
      "metadata": {
        "id": "e2a6f962"
      },
      "outputs": [],
      "source": [
        "# Look at summary statistics\n",
        "df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49959092",
      "metadata": {
        "id": "49959092"
      },
      "source": [
        "Lets plot the distributions of each feature."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c777d3e",
      "metadata": {
        "id": "6c777d3e"
      },
      "outputs": [],
      "source": [
        "# Plot histograms for the non-time features\n",
        "%matplotlib inline\n",
        "\n",
        "pos = []\n",
        "for n in range(5):\n",
        "    for m in range(3):\n",
        "         pos.append((n,m))\n",
        "\n",
        "fig, axs = plt.subplots(5, 3, figsize=(14, 20))\n",
        "features = ['Dom_Pax', 'Int_Pax', 'Pax', 'Dom_Flt', 'Int_Flt', 'Flt', 'Dom_RPM', 'Int_RPM', 'RPM', 'Dom_ASM', 'Int_ASM', 'ASM', 'Dom_LF', 'Int_LF', 'LF']\n",
        "\n",
        "for i in range(15):\n",
        "    col = features[i]\n",
        "    axs[pos[i]].hist(df[col])\n",
        "    axs[pos[i]].set_title(col)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2eabb735",
      "metadata": {
        "id": "2eabb735"
      },
      "source": [
        "All the histograms seems skewed a bit to the left. This is likely due to the year of 2020 during the COVID-19 pandemic when there was much less air traffic than normal. These are likely outliers, and we can deal with them before we create a forecasting model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "090eeeb7",
      "metadata": {
        "id": "090eeeb7"
      },
      "outputs": [],
      "source": [
        "# Plot variables over time\n",
        "fig, axs = plt.subplots(5, 1, figsize=(8, 30))\n",
        "plt.setp(axs, xticks=(range(0, len(df['Date']), 60)), xticklabels=range(2003, 2024, 5))\n",
        "features = ['Pax', 'Flt', 'RPM', 'ASM', 'LF']\n",
        "labels = ['Number of Passengers', 'Number of Flights', 'Revenue Passenger-Miles', 'Available Seat-Miles', 'Load Factor']\n",
        "\n",
        "for i in range(5):\n",
        "  dom = 'Dom_' + features[i]\n",
        "  intl = 'Int_' + features[i]\n",
        "  total = features[i]\n",
        "  axs[i].plot(df['Date'], df[dom], label='Domestic')\n",
        "  axs[i].plot(df['Date'], df[intl], label='International')\n",
        "  axs[i].plot(df['Date'], df[total], label='Total')\n",
        "  axs[i].set_title(labels[i])\n",
        "  axs[i].legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "553151c4",
      "metadata": {
        "id": "553151c4"
      },
      "source": [
        "There are more Domestic flights than International. We can see this in all the variables except Load Factor. We can also again see the effects of the COVID-19 pandemic. Ignoring that time period, we can see that overall the number of flights is going down while all the other variables are going up, including number of passengers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a889aa7d",
      "metadata": {
        "id": "a889aa7d"
      },
      "outputs": [],
      "source": [
        "# Plot variables over time for each year\n",
        "fig, axs = plt.subplots(5, 1, figsize=(12, 50))\n",
        "\n",
        "for n in range(5):\n",
        "    for i in range(21):\n",
        "        axs[n].plot(df['Month'].loc[df['Year'] == i], df[features[n]].loc[df['Year'] == i], label=2003+i)\n",
        "        axs[n].set_title(labels[n])\n",
        "\n",
        "    axs[n].set_xticks(range(1, 13))\n",
        "    axs[n].set_xticklabels(months, rotation=90)\n",
        "    axs[n].legend(bbox_to_anchor=(1.05, 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f50d4db2",
      "metadata": {
        "id": "f50d4db2"
      },
      "source": [
        "In these graphs you can see the overall trends in flights throughout the year. You can see that air travel starts to pick up in the spring and peaks in the summer, dropping off again around September, and finally picking up again during the holidays.\n",
        "\n",
        "These graphs are a bit difficult to read; let's see if we can change the colors to a gradient."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "512fea53",
      "metadata": {
        "id": "512fea53"
      },
      "outputs": [],
      "source": [
        "# Set colormap\n",
        "colors = plt.cm.Blues(np.linspace(0, 1, 21))\n",
        "\n",
        "for n in range(5):\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plt.xticks(range(1, 13), months, rotation='vertical')\n",
        "    plt.title(labels[n])\n",
        "\n",
        "    for i in range(21):\n",
        "        plt.plot(df['Month'].loc[df['Year'] == i], df[features[n]].loc[df['Year'] == i], label=2003+i, color=colors[i])\n",
        "    plt.legend(bbox_to_anchor=(1.05, 1))\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "703c08fe",
      "metadata": {
        "id": "703c08fe"
      },
      "source": [
        "Now we can see the trends over the years. again we see the number of flights goes down over the years while all the other variables increase over the years. We can see where air travel drops off during the COVID-19 pandemic in March/April of 2020 and where it slowly picks up again the following year. We can also see that number of passengers is at a peak this past year (2023), as is Revenue Passenger-Miles and Available Seat-Miles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68b60ecb",
      "metadata": {
        "id": "68b60ecb"
      },
      "outputs": [],
      "source": [
        "# Plot correlation matrix to see which variables are correlated with each other\n",
        "# For this we are only considering the overall variables (i.e., not the Domestic and International variables)\n",
        "matrix = df[['Month', 'Year', 'Pax', 'Flt', 'RPM', 'ASM', 'LF']].corr()\n",
        "\n",
        "plt.imshow(matrix, cmap='Blues')\n",
        "plt.colorbar()\n",
        "\n",
        "variables = []\n",
        "for i in matrix.columns:\n",
        "    variables.append(i)\n",
        "\n",
        "plt.xticks(range(len(matrix)), variables, rotation=45, ha='right')\n",
        "plt.yticks(range(len(matrix)), variables)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4598367",
      "metadata": {
        "id": "b4598367"
      },
      "source": [
        "As we could already kind of see in the previous trends, the number of passengers is correlated to the Revenue Passenger-Miles, the Available Seat-Miles, and the Load Factor. This makes sense since these variables are all dependent on the number of passengers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eaab0e94",
      "metadata": {
        "id": "eaab0e94"
      },
      "outputs": [],
      "source": [
        "years = df['Year'].unique()\n",
        "\n",
        "# Draw Plot\n",
        "fig, axes = plt.subplots(1, 2, figsize=(20,7), dpi= 80)\n",
        "sns.boxplot(x='Year', y='Pax', data=df, ax=axes[0])\n",
        "sns.boxplot(x='Month', y='Pax', data=df.loc[~df['Year'].isin([0, 18]), :])\n",
        "\n",
        "# Set Title\n",
        "axes[0].set_title('Number of Passengers (Trend)', fontsize=18)\n",
        "axes[0].set_xticklabels(range(2003, 2024))\n",
        "axes[0].tick_params(axis='x', labelrotation=90)\n",
        "axes[1].set_title('Number of Passengers (Seasonality)', fontsize=18)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e81126ac",
      "metadata": {
        "id": "e81126ac"
      },
      "source": [
        "Finally, from the boxplots above, we can see that there is a positive trend (other than the years 2020 and 2021) and there is yearly seasonality.\n",
        "\n",
        "## Final Models\n",
        "\n",
        "We want to see if there is a difference in the number of passengers before the COVID-19 pandemic versus after the pandemic. To do this, we will run a paired t-test on the number of passengers in the months before the pandemic and the months after. For this test, we will say that the period of restricted air travel lasted from 2020 through 2021 (it likely lifted sooner, but we will use a wider interval to be safe). We will look at the year before this period (2019) and the year after (2022). The null hypothesis is that there is no difference in the number of passengers before the pandemic and after the pandemic. The alternative hypothesis is that there is a difference."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run t-test to check if the number of passengers in the years before the COVID-19 pandemic is statistically\n",
        "# different from the number of passengers in the years after the pandemic.\n",
        "pre_covid = df[df['Year'] == 16]\n",
        "post_covid = df[df['Year'] == 19]\n",
        "pre_covid = pre_covid[:len(post_covid)]\n",
        "\n",
        "stats.ttest_rel(pre_covid['Pax'], post_covid['Pax'])"
      ],
      "metadata": {
        "id": "clybeao0le1A"
      },
      "id": "clybeao0le1A",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The p-value is less than $\\alpha=0.05$, so we have sufficient evidence to reject the null hypothesis that the number of passengers is the same in these two periods. Therefore, we can conclude that there is a difference in the number of passengers before the pandemic versus after the pandemic. However, we don't know what this difference is.\n",
        "\n",
        "Let's see if we can create a model to forecast the number of passengers in coming months. Before building our model, we have to split the data into train and test sets."
      ],
      "metadata": {
        "id": "6YtITvGAuTNN"
      },
      "id": "6YtITvGAuTNN"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5489685",
      "metadata": {
        "id": "f5489685"
      },
      "outputs": [],
      "source": [
        "# Split into train and test\n",
        "test = df.loc[df['Year'] >= 19]\n",
        "train = df.loc[df['Year'] < 19]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can decompose the data into its three componenets: trend, seasonality, and residual."
      ],
      "metadata": {
        "id": "A552FQzS9VQg"
      },
      "id": "A552FQzS9VQg"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e689e9b",
      "metadata": {
        "id": "0e689e9b"
      },
      "outputs": [],
      "source": [
        "# Additive Decomposition\n",
        "additive_decomposition = seasonal_decompose(train['Pax'], model='additive', period=12)\n",
        "\n",
        "# Multiplicative Decomposition\n",
        "multiplicative_decomposition = seasonal_decompose(train['Pax'], model='multiplicative', period=12)\n",
        "\n",
        "# Plot\n",
        "plt.rcParams.update({'figure.figsize': (16,12)})\n",
        "additive_decomposition.plot().suptitle('Additive Decomposition', fontsize=16)\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "\n",
        "multiplicative_decomposition.plot().suptitle('Multiplicative Decomposition', fontsize=16)\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "While it is hard to tell, the additive decomposition looks a bit more random, so we will use this decomposition to test for outliers in the residuals."
      ],
      "metadata": {
        "id": "hYHHKsU79y9v"
      },
      "id": "hYHHKsU79y9v"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "768d798b",
      "metadata": {
        "id": "768d798b"
      },
      "outputs": [],
      "source": [
        "plt.rc('figure',figsize=(12,6))\n",
        "plt.rc('font',size=15)\n",
        "fig, ax = plt.subplots()\n",
        "x = additive_decomposition.resid.index\n",
        "y = additive_decomposition.resid.values\n",
        "ax.plot(x, y)\n",
        "ax.hlines(y=1e7, xmin=0, xmax=249, color='r')\n",
        "ax.hlines(y=-1e7, xmin=0, xmax=249, color='r')\n",
        "ax.set_xticks((range(0, len(df['Date']), 24)))\n",
        "ax.set_xticklabels(range(2003, 2024, 2))\n",
        "ax.set_title('Residual Outliers')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to deal with the COVID-19 outliers, we are going to try replacing the outliers with the mean number of passengers for that month (not including the outliers in the mean calculation)."
      ],
      "metadata": {
        "id": "5soYyDvj-S0m"
      },
      "id": "5soYyDvj-S0m"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58779b00",
      "metadata": {
        "id": "58779b00"
      },
      "outputs": [],
      "source": [
        "# Convert outliers to the mean for each month (not including the outliers)\n",
        "outliers = list(x[abs(y) > 1e7])\n",
        "do_train = train.drop(labels=outliers)\n",
        "\n",
        "for n in outliers:\n",
        "    month = train.iloc[n]['Month']\n",
        "    train.loc[n, train.columns.get_loc('Pax')] = do_train[do_train['Month'] == month]['Pax'].mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we need to check for stationarity. We do this using the Augmented Dicky-Fuller Test."
      ],
      "metadata": {
        "id": "xcIPS8aW-r1L"
      },
      "id": "xcIPS8aW-r1L"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f8410a2",
      "metadata": {
        "id": "2f8410a2"
      },
      "outputs": [],
      "source": [
        "# Run Augmented Dicky-Fuller Test\n",
        "results = adfuller(train['Pax'])\n",
        "print(f'ADF Statistic: {results[0]}')\n",
        "print(f'p-value: {results[1]}')\n",
        "print('Critical Values:')\n",
        "for key, value in results[4].items():\n",
        "    print(f'{key}: {value}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our p-value for this test is less than $\\alpha=0.05$, but the ADF statistic isn't significantly smaller than the critical values. Let's see what happens when we difference the data."
      ],
      "metadata": {
        "id": "2E7W7Kfv-yij"
      },
      "id": "2E7W7Kfv-yij"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "944318c8",
      "metadata": {
        "id": "944318c8"
      },
      "outputs": [],
      "source": [
        "# Transform time series by differencing\n",
        "pax_transformed = train['Pax'].diff().dropna()\n",
        "results = adfuller(pax_transformed)\n",
        "print(f'ADF Statistic: {results[0]}')\n",
        "print(f'p-value: {results[1]}')\n",
        "print('Critical Values:')\n",
        "for key, value in results[4].items():\n",
        "    print(f'{key}: {value}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This seems much better. Now let's plot the Autocorrelation and Partial Autocorrelation functions."
      ],
      "metadata": {
        "id": "SeI_nyH8R5C-"
      },
      "id": "SeI_nyH8R5C-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4588c7d2",
      "metadata": {
        "id": "4588c7d2"
      },
      "outputs": [],
      "source": [
        "# Plot Autocorrelation\n",
        "plot_acf(train['Pax'], lags=50)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3e6b97f",
      "metadata": {
        "id": "a3e6b97f"
      },
      "outputs": [],
      "source": [
        "# Plot Partial Autocorrelation\n",
        "plot_pacf(train['Pax'], lags=50)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that there is definitely yearly seasonality in the data. We can use these plots to determine $p$ and $q$, or we can try a grid search. This way tests out all the combinations of different hyperparameters to find the best combination based on the AIC. This will likely give us better results, so let's do this."
      ],
      "metadata": {
        "id": "jOyb-7EjWRss"
      },
      "id": "jOyb-7EjWRss"
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize parameters\n",
        "p_params = range(0, 3)\n",
        "d_params = [0, 1]\n",
        "q_params = range(0, 3)\n",
        "P_params = range(0, 3)\n",
        "D_params = [0, 1]\n",
        "Q_params = range(0, 3)\n",
        "\n",
        "params = {'order': [], 'seasonal_order': []}\n",
        "for p in p_params:\n",
        "    for d in d_params:\n",
        "        for q in q_params:\n",
        "            order = (p, d, q)\n",
        "            params['order'].append(order)\n",
        "\n",
        "for P in P_params:\n",
        "    for D in D_params:\n",
        "        for Q in Q_params:\n",
        "            seasonal_order = (P, D, Q, 12)\n",
        "            params['seasonal_order'].append(seasonal_order)\n",
        "\n",
        "best_aic = 100000\n",
        "best_params = None\n",
        "for order in params['order']:\n",
        "    for seasonal_order in params['seasonal_order']:\n",
        "        model = SARIMAX(train['Pax'], order=order, seasonal_order=seasonal_order)\n",
        "        model_fit = model.fit()\n",
        "        aic = model_fit.aic\n",
        "        preds = model_fit.predict()\n",
        "        if aic < best_aic:\n",
        "            best_aic = aic\n",
        "            best_params = [order, seasonal_order]\n",
        "\n",
        "print(f'Best AIC: {best_aic}')\n",
        "print(f'Best parameters: {best_params}')"
      ],
      "metadata": {
        "id": "njbak7hZSzWg"
      },
      "id": "njbak7hZSzWg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the grid search, we found that the best combination is $order=(2, 1, 0)$ and $seasonal\\:order=(0, 1, 1, 12)$. Let's try building our SARIMA model with these parameters."
      ],
      "metadata": {
        "id": "OP4Il-EHXDNr"
      },
      "id": "OP4Il-EHXDNr"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff6819cd",
      "metadata": {
        "id": "ff6819cd"
      },
      "outputs": [],
      "source": [
        "# Build SARIMA model\n",
        "model = SARIMAX(train['Pax'], order=(2, 1, 0), seasonal_order=(0, 1, 1, 12))\n",
        "model_fit = model.fit()\n",
        "preds = model_fit.predict()\n",
        "model_fit.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb368e2e",
      "metadata": {
        "id": "cb368e2e"
      },
      "outputs": [],
      "source": [
        "# Plot residual errors\n",
        "residuals = pd.DataFrame(model_fit.resid)\n",
        "fig, ax = plt.subplots(1,2)\n",
        "residuals.plot(title=\"Residuals\", ax=ax[0])\n",
        "residuals.plot(kind='kde', title='Density', ax=ax[1])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f97d4fb",
      "metadata": {
        "id": "4f97d4fb"
      },
      "outputs": [],
      "source": [
        "# Plot Actual vs Fitted\n",
        "fig, ax = plt.subplots(figsize=(10, 7))\n",
        "ax = train['Pax'].plot(ax=ax)\n",
        "ax.plot(test['Pax'])\n",
        "ax.set_xticks((range(0, len(df['Date']), 48)))\n",
        "ax.set_xticklabels(range(2003, 2024, 4))\n",
        "plot_predict(model_fit, 175, 249, ax=ax)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Train MAPE: {mean_absolute_percentage_error(train['Pax'], preds)}\")\n",
        "preds = model_fit.predict(229, 249)\n",
        "print(f\"Test MAPE: {mean_absolute_percentage_error(test['Pax'], preds)}\")"
      ],
      "metadata": {
        "id": "haWLQQJqhOi9"
      },
      "id": "haWLQQJqhOi9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This model seems to do pretty well on the training data. The mean absolute percentage error is small, indicating the model is performing very well. The residuals are pretty random, and the forecast is fairly close to the actual data, as seen in the plot above.\n",
        "\n",
        "The model also does well on the test data, but is not as accurate as we would like. There is also a large confidence interval. Let's try one more model to see if we can do better. For the second model, we are going to try an LSTM (Long-Short Term Memory) neural network. First, we have to do a little preprocessing to the data. LSTMs are very sensitive to the scale of input data, so we normalize the data so that it is in the range $[0, 1]$."
      ],
      "metadata": {
        "id": "UTZxESZLZIxI"
      },
      "id": "UTZxESZLZIxI"
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(12)\n",
        "\n",
        "dataframe = pd.concat([train['Pax'], test['Pax']])\n",
        "dataset = dataframe.values\n",
        "dataset = dataset.astype('float32')\n",
        "dataset = dataset.reshape(-1, 1)"
      ],
      "metadata": {
        "id": "KumP4MREZaan"
      },
      "id": "KumP4MREZaan",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize the dataset\n",
        "scaler = MinMaxScaler()\n",
        "dataset = scaler.fit_transform(dataset)"
      ],
      "metadata": {
        "id": "gZqRl5ZgaQIO"
      },
      "id": "gZqRl5ZgaQIO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split into train and test\n",
        "train, test = dataset[0:228,:], dataset[228:len(dataset),:]"
      ],
      "metadata": {
        "id": "OPs97rrbaW9I"
      },
      "id": "OPs97rrbaW9I",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We create a function that turns the data into two arrays, the first array being the number of passengers of the current month, and the second array is the number of passengers of the next month. This is how we will feed the data into the LSTM model."
      ],
      "metadata": {
        "id": "f6N4LQHpgCGd"
      },
      "id": "f6N4LQHpgCGd"
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert an array of values into a dataset matrix\n",
        "def create_dataset(dataset, look_back=1):\n",
        "    dataX, dataY = [], []\n",
        "\n",
        "    for i in range(len(dataset)-look_back-1):\n",
        "      a = dataset[i:(i+look_back), 0]\n",
        "      dataX.append(a)\n",
        "      dataY.append(dataset[i + look_back, 0])\n",
        "\n",
        "    return np.array(dataX), np.array(dataY)"
      ],
      "metadata": {
        "id": "87aw0fFBaX99"
      },
      "id": "87aw0fFBaX99",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshape into X=t and Y=t+1\n",
        "look_back = 1\n",
        "trainX, trainY = create_dataset(train, look_back)\n",
        "testX, testY = create_dataset(test, look_back)"
      ],
      "metadata": {
        "id": "x-PxoXA_ajy7"
      },
      "id": "x-PxoXA_ajy7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshape input to be [samples, time steps, features]\n",
        "trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
        "testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))"
      ],
      "metadata": {
        "id": "aXAE6Heqanze"
      },
      "id": "aXAE6Heqanze",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that the data is formatted correctly, we can create and fit our LSTM model."
      ],
      "metadata": {
        "id": "X4WC2b-dga8j"
      },
      "id": "X4WC2b-dga8j"
    },
    {
      "cell_type": "code",
      "source": [
        "# Create and fit the LSTM network\n",
        "model = Sequential()\n",
        "model.add(LSTM(4, input_shape=(1, look_back)))\n",
        "model.add(Dense(1))\n",
        "model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "history = model.fit(trainX, trainY, epochs=100, batch_size=1, verbose=2)"
      ],
      "metadata": {
        "id": "aBi1KMf_arNy"
      },
      "id": "aBi1KMf_arNy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see how it does on the train and test sets."
      ],
      "metadata": {
        "id": "vPvSNRBWgliB"
      },
      "id": "vPvSNRBWgliB"
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions\n",
        "trainPredict = model.predict(trainX)\n",
        "testPredict = model.predict(testX)\n",
        "\n",
        "# Invert predictions\n",
        "trainPredict = scaler.inverse_transform(trainPredict)\n",
        "trainY = scaler.inverse_transform([trainY])\n",
        "testPredict = scaler.inverse_transform(testPredict)\n",
        "testY = scaler.inverse_transform([testY])\n",
        "\n",
        "# Calculate mean absolute percentage error\n",
        "print(f\"Train MAPE: {mean_absolute_percentage_error(trainY[0], trainPredict[:,0])}\")\n",
        "print(f\"Test MAPE: {mean_absolute_percentage_error(testY[0], testPredict[:,0])}\")"
      ],
      "metadata": {
        "id": "JpdaDAnaaubO"
      },
      "id": "JpdaDAnaaubO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Shift train predictions for plotting\n",
        "trainPredictPlot = np.empty_like(dataset)\n",
        "trainPredictPlot[:, :] = np.nan\n",
        "trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n",
        "\n",
        "# Shift test predictions for plotting\n",
        "testPredictPlot = np.empty_like(dataset)\n",
        "testPredictPlot[:, :] = np.nan\n",
        "testPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict\n",
        "\n",
        "# Plot baseline and predictions\n",
        "fig, ax = plt.subplots()\n",
        "a = ax.plot(scaler.inverse_transform(dataset))\n",
        "b = ax.plot(trainPredictPlot)\n",
        "c = ax.plot(testPredictPlot)\n",
        "ax.set_xticks((range(0, len(df['Date']), 48)))\n",
        "ax.set_xticklabels(range(2003, 2024, 4))\n",
        "ax.legend([a, b, c], labels=['Actual', 'Train Forecast', 'Test Forecast'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FkZnmRMHa1HM"
      },
      "id": "FkZnmRMHa1HM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_loss(history):\n",
        "    plt.figure(figsize=(10, 7))\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.title('Model Loss')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.show()\n",
        "\n",
        "plot_loss(history)"
      ],
      "metadata": {
        "id": "04fSkEnSeZvR"
      },
      "id": "04fSkEnSeZvR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This model seems to do pretty well on our data. The mean absolute percentage error is about 13% for the training data, which is good, and about 6.7% for the test data, which is excellent. The difference in these values (normally the train would be smaller) might be due to the size of my test set. It might have been better to test on a larger sample, but I didn't want the COVID-19 data to be in the test set. We also plotted the loss to make sure the model is converging, which it is.\n",
        "\n",
        "## Final Analysis\n",
        "\n",
        "Both of these forecasting models do a pretty good job at predicting the Number of Passengers. The LSTM model does a bit better on the test dataset. One interesting thing we noticed is that the model's prediction on the test set is lower than the actual values for both models. This may indicate that there is, in fact, an increase in air travel that even the model might not realize due to the fact that it was trained on past data.\n",
        "\n",
        "Another interesting thing we found in our EDA is that the number of flights decreases over the years while the number of passengers increases. This might mean that airplanes are able to hold more passengers, or that flights are more often fully booked. We also noticed that there are much less international flights than domestic flights. Finally, we saw the yearly trends of the data, noticing how the number of passengers increases in the spring and summer as people begin traveling, and then drops off around September when kids are starting to go back to school. There is another increase around Christmas, as you'd expect.\n",
        "\n",
        "Our paired t-test showed that there is a difference in the Number of Passengers in the period before the COVID-19 pandemic versus the period after the pandemic. From the EDA and the forecasting predictions, it seems that this difference is an increase.\n"
      ],
      "metadata": {
        "id": "-KgjmQtrgqmE"
      },
      "id": "-KgjmQtrgqmE"
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab2PDF v1.0.4 by Drengskapur (github.com/drengskapur/colab2pdf) (License: GPL-3.0-or-later)\n",
        "# @title {display-mode:\"form\"}\n",
        "# @markdown ⬇️ Download PDF\n",
        "def colab2pdf():\n",
        "    ENABLE=True # @param {type:\"boolean\"}\n",
        "    if ENABLE:\n",
        "        !apt-get install librsvg2-bin\n",
        "        import os, datetime, json, locale, pathlib, urllib, requests, werkzeug, nbformat, google, yaml, warnings\n",
        "        locale.setlocale(locale.LC_ALL, 'en_US.UTF-8')\n",
        "        NAME = pathlib.Path(werkzeug.utils.secure_filename(urllib.parse.unquote(requests.get(f\"http://{os.environ['COLAB_JUPYTER_IP']}:{os.environ['KMP_TARGET_PORT']}/api/sessions\").json()[0][\"name\"])))\n",
        "        TEMP = pathlib.Path(\"/content/pdfs\") / f\"{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}_{NAME.stem}\"; TEMP.mkdir(parents=True, exist_ok=True)\n",
        "        NB = [cell for cell in nbformat.reads(json.dumps(google.colab._message.blocking_request(\"get_ipynb\", timeout_sec=600)[\"ipynb\"]), as_version=4).cells if \"--Colab2PDF\" not in cell.source]\n",
        "        warnings.filterwarnings('ignore', category=nbformat.validator.MissingIDFieldWarning)\n",
        "        with (TEMP / f\"{NAME.stem}.ipynb\").open(\"w\", encoding=\"utf-8\") as nb_copy: nbformat.write(nbformat.v4.new_notebook(cells=NB or [nbformat.v4.new_code_cell(\"#\")]), nb_copy)\n",
        "        if not pathlib.Path(\"/usr/local/bin/quarto\").exists():\n",
        "            !wget -q \"https://quarto.org/download/latest/quarto-linux-amd64.deb\" -P {TEMP} && dpkg -i {TEMP}/quarto-linux-amd64.deb > /dev/null && quarto install tinytex --update-path --quiet\n",
        "        with (TEMP / \"config.yml\").open(\"w\", encoding=\"utf-8\") as file: yaml.dump({'include-in-header': [{\"text\": r\"\\usepackage{fvextra}\\DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaksymbolleft={},showspaces=false,showtabs=false,breaklines,breakanywhere,commandchars=\\\\\\{\\}}\"}],'include-before-body': [{\"text\": r\"\\DefineVerbatimEnvironment{verbatim}{Verbatim}{breaksymbolleft={},showspaces=false,showtabs=false,breaklines}\"}]}, file)\n",
        "        !quarto render {TEMP}/{NAME.stem}.ipynb --metadata-file={TEMP}/config.yml --to pdf -M latex-auto-install -M margin-top=1in -M margin-bottom=1in -M margin-left=1in -M margin-right=1in --quiet\n",
        "        google.colab.files.download(str(TEMP / f\"{NAME.stem}.pdf\"))\n",
        "colab2pdf()"
      ],
      "metadata": {
        "id": "cP2c37M73FFd"
      },
      "id": "cP2c37M73FFd",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}